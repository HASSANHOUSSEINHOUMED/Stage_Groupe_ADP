{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CR4FBTc9opp"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialisation de SparkSession pour activer le support de Hive\n",
        "spark = SparkSession.builder.appName(\"DataFetchWithFluxTracking\").enableHiveSupport().getOrCreate()\n",
        "\n",
        "# Vérifie si la table existe dans Hive et la crée si nécessaire\n",
        "spark.sql(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS c_tech.offset_update (\n",
        "    date STRING,\n",
        "    flux STRING,\n",
        "    last_successful_offset INT,\n",
        "    success BOOLEAN\n",
        ")\"\"\")\n",
        "\n",
        "# Fonction pour logger les messages avec un horodatage\n",
        "def log(message):\n",
        "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {message}\")\n",
        "\n",
        "# Fonction pour obtenir un token d'authentification de l'API\n",
        "def get_bearer_token(auth_url, client_id, client_secret, audience, scope=\"read\", retries=2):\n",
        "    while retries > 0:\n",
        "        try:\n",
        "            # Préparation de la requête pour l'obtention du token\n",
        "            payload = {\"audience\": audience, \"grant_type\": \"client_credentials\", \"client_id\": client_id, \"client_secret\": client_secret, \"scope\": scope}\n",
        "            headers = {\"Content-Type\": \"application/json\"}\n",
        "            # Envoi de la requête\n",
        "            response = requests.post(auth_url, json=payload, headers=headers)\n",
        "            # Vérification du statut de la réponse\n",
        "            response.raise_for_status()\n",
        "            # Retourne le token si la requête est réussie\n",
        "            return response.json().get('access_token')\n",
        "        except Exception as e:\n",
        "            # Log des erreurs en cas de problème lors de l'obtention du token\n",
        "            log(f\"Erreur lors de l'obtention du token: {e}. Tentatives restantes : {retries}\")\n",
        "            retries -= 1\n",
        "    log(\"Échec de l'obtention d'un nouveau token après plusieurs tentatives.\")\n",
        "    return None\n",
        "\n",
        "# Fonction pour extraire le flux (nom de la ressource) à partir de l'URL de l'API\n",
        "def extract_flux_from_url(url):\n",
        "    segments = url.strip('/').split('/')\n",
        "    return segments[4] if len(segments) >= 4 else None\n",
        "\n",
        "# Fonction pour obtenir le dernier offset et date d'échec enregistré dans Hive\n",
        "def get_last_failure_info(flux):\n",
        "    \"\"\"Fonction pour obtenir les informations sur le dernier échec enregistré dans Hive.\"\"\"\n",
        "    try:\n",
        "        query_result = spark.sql(f\"\"\"\n",
        "            SELECT date, last_successful_offset\n",
        "            FROM c_tech.offset_update\n",
        "            WHERE flux = '{flux}' AND success = FALSE\n",
        "            ORDER BY date ASC, last_successful_offset ASC\n",
        "            LIMIT 1\n",
        "        \"\"\").collect()\n",
        "        if query_result:\n",
        "            return query_result[0]['date'], query_result[0]['last_successful_offset']\n",
        "        return None, 0\n",
        "    except Exception as e:\n",
        "        log(f\"Erreur lors de la récupération des informations sur le dernier échec : {e}\")\n",
        "        return None, 0\n",
        "\n",
        "# Fonction pour mettre à jour le suivi des offsets dans Hive\n",
        "def update_offset_tracking(date_str, flux, offset, success):\n",
        "    try:\n",
        "        # Insère l'offset actuel dans la table si l'opération est un échec\n",
        "        if not success:\n",
        "            spark.sql(f\"INSERT INTO TABLE c_tech.offset_update VALUES ('{date_str}', '{flux}', {offset}, {success})\")\n",
        "    except Exception as e:\n",
        "        log(f\"Erreur lors de la mise à jour du suivi des offsets : {e}\")\n",
        "\n",
        "# Fonction principale pour récupérer les données de l'API et les sauvegarder localement\n",
        "def fetch_data_api(base_url, save_directory, auth_url, client_id, client_secret, audience, user_defined_limit, scope=\"read\"):\n",
        "    flux = extract_flux_from_url(base_url)\n",
        "    if not flux:\n",
        "        log(\"Impossible de déterminer le flux à partir de l'URL.\")\n",
        "        return\n",
        "\n",
        "    date_str, last_failure_offset = get_last_failure_info(flux)\n",
        "    if not date_str:\n",
        "        date_str = datetime.now().strftime(\"%Y%m%d\")\n",
        "\n",
        "    output_filename = flux  # Utilise le flux comme nom pour le fichier de sortie\n",
        "    token = get_bearer_token(auth_url, client_id, client_secret, audience, scope)\n",
        "    if not token:\n",
        "        return  # Quitte si l'obtention du token échoue\n",
        "\n",
        "    offset = last_failure_offset if last_failure_offset is not None else 0\n",
        "    file_number = offset // user_defined_limit if offset else 0\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "            response = requests.get(base_url, headers=headers, params={\"limit\": user_defined_limit, \"offset\": offset})\n",
        "            if response.status_code == 401:  # Token expiré\n",
        "                log(\"Le token a expiré, obtention d'un nouveau...\")\n",
        "                token = get_bearer_token(auth_url, client_id, client_secret, audience, scope)\n",
        "                if not token:\n",
        "                    break  # Quitte si un nouveau token ne peut pas être obtenu\n",
        "                continue  # Réessaye avec le nouveau token\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                total_count = data.get(\"meta\", {}).get(\"total_count\", 0)\n",
        "                limit = data.get(\"meta\", {}).get(\"limit\", user_defined_limit)\n",
        "\n",
        "                # Arrêter le script si total_count est 0 dès le début\n",
        "                if total_count == 0 and offset == 0:\n",
        "                    log(\"Aucune donnée à récupérer.\")\n",
        "                    break\n",
        "\n",
        "                # Création du nom du fichier et sauvegarde des données\n",
        "                formatted_filename = f\"{output_filename}_{date_str}_{file_number}.json\"\n",
        "                file_path = f\"{save_directory}/{formatted_filename}\"\n",
        "                with open(file_path, \"w\") as file:\n",
        "                    json.dump(data, file)\n",
        "                log(f\"Données sauvegardées dans {file_path}\")\n",
        "\n",
        "                # Mise à jour des variables pour la prochaine itération\n",
        "                offset += limit\n",
        "                file_number += 1\n",
        "                if offset > total_count:\n",
        "                    log(\"Toutes les données ont été récupérées avec succès.\")\n",
        "                    if last_failure_offset is not None:\n",
        "                       update_offset_tracking(date_str, flux, last_failure_offset, True)\n",
        "\n",
        "                    break\n",
        "            else:\n",
        "                log(f\"Erreur avec le code de réponse {response.status_code}\")\n",
        "                update_offset_tracking(date_str, flux, offset, False)\n",
        "                break\n",
        "        except Exception as e:\n",
        "            log(f\"Erreur lors de la récupération des données : {e}\")\n",
        "            update_offset_tracking(date_str, flux, offset, False)\n",
        "            break"
      ]
    }
  ]
}