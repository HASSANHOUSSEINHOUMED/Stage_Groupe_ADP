{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzFmnGZp8q8h"
      },
      "outputs": [],
      "source": [
        "pip install openpyxl\n",
        "dbutils.library.restartPython()\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "# Initialiser SparkSession avec le support Hive\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Skytrax_Orly\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "# Créer ou accéder à une table Hive pour stocker les chemins des fichiers traités\n",
        "spark.sql(\"CREATE TABLE IF NOT EXISTS c_tech.skytrax_treated_files_orly (nom_fichier STRING)\")\n",
        "# Fonction pour ajouter les données à chaque DataFrame selon le périmètre\n",
        "def add_data_to_df(subtheme, data, perimeter, theme, file_name):\n",
        "    if perimeter == 'landside':\n",
        "        # Sauter la première ligne et prendre les 5 premières colonnes\n",
        "        temp_df = data.iloc[1:, :5].copy()\n",
        "        temp_df.columns = ['Details', 'ORY1', 'ORY2', 'ORY3', 'ORY4']\n",
        "    elif perimeter == 'airside':\n",
        "        # Sauter la première ligne et prendre les 7 premières colonnes\n",
        "        temp_df = data.iloc[1:, :7].copy()\n",
        "        temp_df.columns = ['Details', 'Gates A', 'Gates B', 'Gates C', 'Gates D', 'Gates E', 'Gates F']\n",
        "    elif perimeter == 'overall':\n",
        "        # Sauter la première ligne et prendre les 2 premières colonnes\n",
        "        temp_df = data.iloc[1:, :2].copy()\n",
        "        temp_df.columns = ['Details', 'Rates']\n",
        "    # Ajouter les colonnes communes\n",
        "    temp_df['Subtheme'] = subtheme\n",
        "    temp_df['Theme'] = theme\n",
        "    temp_df['Plateforme'] = plateforme\n",
        "    temp_df['Perimetre'] = perimeter\n",
        "    temp_df['Year'] = year\n",
        "    temp_df['Fichier_Source'] = file_name\n",
        "\n",
        "    # Concaténer avec le DataFrame correspondant\n",
        "    if perimeter == 'landside':\n",
        "        global df_landside\n",
        "        df_landside = pd.concat([df_landside, temp_df], ignore_index=True)\n",
        "    elif perimeter == 'airside':\n",
        "        global df_airside\n",
        "        df_airside = pd.concat([df_airside, temp_df], ignore_index=True)\n",
        "    elif perimeter == 'overall':\n",
        "        global df_overall\n",
        "        df_overall = pd.concat([df_overall, temp_df], ignore_index=True)\n",
        "# Chemin du dossier contenant les fichiers CSV\n",
        "dossier = '/dbfs/FileStore/Extime Patrimony/AERO/Raw/Skytrax/ORLY'\n",
        "\n",
        "# Liste des noms de fichiers\n",
        "file_names = [fichier for fichier in os.listdir(dossier) if fichier.endswith('.xlsx')]\n",
        "\n",
        "# Identifier les nouveaux fichiers\n",
        "# cette liste contiendra les noms de fichiers uniquement\n",
        "nouveaux_fichiers = []\n",
        "for file_name in file_names:\n",
        "    # Vérifier si le nom du fichier est déjà dans la table Hive\n",
        "    if not spark.sql(f\"SELECT nom_fichier FROM c_tech.skytrax_treated_files_orly WHERE nom_fichier = '{file_name}'\").collect():\n",
        "        nouveaux_fichiers.append(file_name)\n",
        "# Gestion des tables d'archivage\n",
        "if nouveaux_fichiers:\n",
        "    spark.sql(\"CREATE TABLE IF NOT EXISTS a_ing.Skytrax_ORY_Rating_landside LIKE c_ing.Skytrax_ORY_Rating_landside\")\n",
        "    spark.sql(\"CREATE TABLE IF NOT EXISTS a_ing.Skytrax_ORY_Rating_airside LIKE c_ing.Skytrax_ORY_Rating_airside\")\n",
        "    spark.sql(\"CREATE TABLE IF NOT EXISTS a_ing.Skytrax_ORY_Rating_overall LIKE c_ing.Skytrax_ORY_Rating_overall\")\n",
        "\n",
        "    spark.sql(\"CREATE TABLE IF NOT EXISTS a_ing.Skytrax_ORY_Rating_landside_overall LIKE c_ing.Skytrax_ORY_Rating_landside_overall\")\n",
        "    spark.sql(\"CREATE TABLE IF NOT EXISTS a_ing.Skytrax_ORY_Rating_airside_overall LIKE c_ing.Skytrax_ORY_Rating_airside_overall\")\n",
        "    spark.sql(\"CREATE TABLE IF NOT EXISTS a_ing.Skytrax_ORY_Rating_overall_overall LIKE c_ing.Skytrax_ORY_Rating_overall_overall\")\n",
        "\n",
        "    spark.sql(\"CREATE TABLE IF NOT EXISTS a_exp.Skytrax_Rating LIKE c_exp.Skytrax_Rating\")\n",
        "\n",
        "    spark.sql(\"CREATE TABLE IF NOT EXISTS a_rep.Skytrax_Rating LIKE c_rep.Skytrax_Rating\")\n",
        "else:\n",
        "    print(\"Aucun nouveau  fichier dans cette repertoire, veuillez réessayer utlterièrement\")\n",
        "if nouveaux_fichiers:\n",
        "   # Insertion des anciens tables dans l'archive\n",
        " spark.sql(\"\"\"\n",
        "      INSERT INTO a_ing.Skytrax_ORY_Rating_landside\n",
        "      SELECT * FROM c_ing.Skytrax_ORY_Rating_landside\n",
        "      where Year NOT IN (SELECT Year FROM a_ing.Skytrax_ORY_Rating_landside)\n",
        "    \"\"\")\n",
        " spark.sql(\"\"\"\n",
        "      INSERT INTO a_ing.Skytrax_ORY_Rating_airside\n",
        "      SELECT * FROM c_ing.Skytrax_ORY_Rating_airside\n",
        "      where Year NOT IN (SELECT Year FROM a_ing.Skytrax_ORY_Rating_airside)\n",
        "   \"\"\")\n",
        " spark.sql(\"\"\"\n",
        "      INSERT INTO a_ing.Skytrax_ORY_Rating_overall\n",
        "      SELECT * FROM c_ing.Skytrax_ORY_Rating_overall\n",
        "      where Year NOT IN (SELECT Year FROM a_ing.Skytrax_ORY_Rating_overall)\n",
        "   \"\"\")\n",
        " spark.sql(\"\"\"\n",
        "      INSERT INTO a_ing.Skytrax_ORY_Rating_landside_overall\n",
        "      SELECT * FROM c_ing.skytrax_ory_rating_landside_overall\n",
        "      where Year NOT IN (SELECT Year FROM a_ing.Skytrax_ORY_Rating_landside_overall)\n",
        "   \"\"\")\n",
        " spark.sql(\"\"\"\n",
        "      INSERT INTO a_ing.Skytrax_ORY_Rating_airside_overall\n",
        "      SELECT * FROM c_ing.Skytrax_ORY_Rating_airside_overall\n",
        "      where Year NOT IN (SELECT Year FROM a_ing.Skytrax_ORY_Rating_airside_overall)\n",
        "   \"\"\")\n",
        " spark.sql(\"\"\"\n",
        "      INSERT INTO a_ing.Skytrax_ORY_Rating_overall_overall\n",
        "      SELECT * FROM c_ing.Skytrax_ORY_Rating_overall_overall\n",
        "      where Year NOT IN (SELECT Year FROM a_ing.Skytrax_ORY_Rating_overall_overall)\n",
        "   \"\"\")\n",
        " spark.sql(\"\"\"\n",
        "      INSERT INTO a_exp.Skytrax_Rating\n",
        "      SELECT * FROM c_exp.Skytrax_Rating\n",
        "      where Year NOT IN (SELECT Year FROM a_exp.Skytrax_Rating) AND Plateforme NOT IN (SELECT Plateforme FROM a_exp.Skytrax_Rating)\n",
        "   \"\"\")\n",
        " spark.sql(\"\"\"\n",
        "      INSERT INTO a_rep.Skytrax_Rating\n",
        "      SELECT * FROM c_rep.Skytrax_Rating\n",
        "      where Year NOT IN (SELECT Year FROM a_rep.Skytrax_Rating) AND Plateforme NOT IN (SELECT Plateforme FROM a_rep.Skytrax_Rating)\n",
        "   \"\"\")\n",
        "else:\n",
        "   print(\"Aucun nouveau  fichier dans cette repertoire, veuillez réessayer utlterièrement\")\n",
        "if nouveaux_fichiers:\n",
        "    for nom_fichier in nouveaux_fichiers:\n",
        "     # Reconstruire le chemin complet\n",
        "     file_path = os.path.join(dossier, nom_fichier)\n",
        "\n",
        "     file_name= os.path.basename(file_path)\n",
        "     skytrax = pd.ExcelFile(file_path)\n",
        "\n",
        "     parts = file_name.split('_')\n",
        "\n",
        "     plateforme = parts[0]\n",
        "     year = parts[3]\n",
        "\n",
        "     if plateforme == \"ORY\":\n",
        "          plateforme = \"ORLY\"\n",
        "\n",
        "     # Preparation de Table Row\n",
        "\n",
        "     # Traitement de l'onglet Overall\n",
        "     for sheet_name in skytrax.sheet_names:\n",
        "         if sheet_name == \"Overall\":\n",
        "            # Lire l'onglet dans un DataFrame\n",
        "             df = skytrax.parse(sheet_name,header=None)\n",
        "             df = df.dropna(axis=0, how='all')\n",
        "             df = df.reset_index(drop=True)\n",
        "\n",
        "             theme = sheet_name\n",
        "\n",
        "             # Initialisation des listes pour stocker les indices de lignes\n",
        "             five_columns_indices = []\n",
        "             seven_columns_indices = []\n",
        "             two_columns_indices = []\n",
        "\n",
        "             # Trouver les indices des lignes contenant \"Aggregate\" pour délimiter les tables\n",
        "             aggregate_indices = df.index[df[0].str.contains(\"Aggregate\", na=False)].tolist()\n",
        "\n",
        "             # Initialiser un index de début à 0 pour la première table\n",
        "             start_index = 0\n",
        "\n",
        "             # Listes pour stocker les DataFrames de chaque table\n",
        "             tables = []\n",
        "\n",
        "             for end_index in aggregate_indices:\n",
        "                 # Extraire chaque table jusqu'à \"Aggregate\" inclus\n",
        "                 table = df.iloc[start_index:end_index+1].reset_index(drop=True)\n",
        "                 tables.append(table)\n",
        "\n",
        "                 # Mettre à jour le start_index pour la prochaine table\n",
        "                 start_index = end_index + 1  # +1 pour exclure la ligne aggregate pour le debut du prochain table\n",
        "\n",
        "             df_five_columns= tables[0]\n",
        "             df_seven_columns= tables[1]\n",
        "             df_two_columns= tables[2]\n",
        "\n",
        "             df_five_columns = df_five_columns.drop(df_five_columns.columns[5:7], axis=1)\n",
        "             df_two_columns = df_two_columns.drop(df_two_columns.columns[2:7], axis=1)\n",
        "\n",
        "             # Renommer les colonnes\n",
        "             df_five_columns.columns = ['Details', 'ORY1', 'ORY2', 'ORY3', 'ORY4']\n",
        "\n",
        "             # Sauter la première ligne et prendre jusqu'à l'avant-dernière ligne\n",
        "             df_landside_overall = df_five_columns.iloc[1:-1].reset_index(drop=True)\n",
        "\n",
        "             # Renommer les colonnes\n",
        "             df_seven_columns.columns = ['Details', 'Gates A', 'Gates B', 'Gates C', 'Gates D', 'Gates E', 'Gates F']\n",
        "\n",
        "             # Sauter la première ligne et prendre jusqu'à l'avant-dernière ligne\n",
        "             df_airside_overall = df_seven_columns.iloc[1:-1].reset_index(drop=True)\n",
        "\n",
        "             # Renommer les colonnes\n",
        "             df_two_columns.columns = ['Details', 'Rates']\n",
        "\n",
        "             # Sauter la première ligne et prendre jusqu'à l'avant-dernière ligne\n",
        "             df_overall_overall = df_two_columns.iloc[2:-1].reset_index(drop=True)\n",
        "\n",
        "             df_landside_overall['Theme'] = theme\n",
        "             df_landside_overall['Plateforme'] = plateforme\n",
        "             df_landside_overall['Perimetre'] = 'landside'\n",
        "             df_landside_overall['Year'] = year\n",
        "             df_landside_overall['Fichier_Source'] = file_name\n",
        "\n",
        "             df_airside_overall['Theme'] = theme\n",
        "             df_airside_overall['Plateforme'] = plateforme\n",
        "             df_airside_overall['Perimetre'] = 'Airside'\n",
        "             df_airside_overall['Year'] = year\n",
        "             df_airside_overall['Fichier_Source'] = file_name\n",
        "\n",
        "             df_overall_overall['Theme'] = theme\n",
        "             df_overall_overall['Plateforme'] = plateforme\n",
        "             df_overall_overall['Perimetre'] = 'Overall'\n",
        "             df_overall_overall['Year'] = year\n",
        "             df_overall_overall['Fichier_Source'] = file_name\n",
        "\n",
        "             landside_subthemes = list(df_landside_overall['Details'])\n",
        "             landside_subthemes = [mot.strip() for mot in landside_subthemes]\n",
        "             airside_subthemes = list(df_airside_overall['Details'])\n",
        "             airside_subthemes = [mot.strip() for mot in airside_subthemes]\n",
        "             overall_subthemes = list(df_overall_overall['Details'])\n",
        "             overall_subthemes = [mot.strip() for mot in overall_subthemes]\n",
        "\n",
        "      # Initialisation des DataFrames pour chaque périmètre\n",
        "     df_landside = pd.DataFrame(columns=['Details', 'ORY1', 'ORY2', 'ORY3', 'ORY4', 'Subtheme', 'Theme', 'Plateforme', 'Perimetre', 'Year', 'Fichier_Source'])\n",
        "     df_airside = pd.DataFrame(columns=['Details', 'Gates A', 'Gates B', 'Gates C', 'Gates D', 'Gates E', 'Gates F', 'Subtheme', 'Theme', 'Plateforme', 'Perimetre', 'Year', 'Fichier_Source'])\n",
        "     df_overall = pd.DataFrame(columns=['Details', 'Rates', 'Subtheme', 'Theme', 'Plateforme', 'Perimetre', 'Year', 'Fichier_Source'])\n",
        "\n",
        "     # Traitement des tous les autres onglets\n",
        "     for sheet_name in skytrax.sheet_names:\n",
        "          if sheet_name != \"Overall\":\n",
        "              # Lire l'onglet dans un DataFrame\n",
        "             df = skytrax.parse(sheet_name,header=None)\n",
        "             df = df.dropna(axis=0, how='all')\n",
        "             df = df.reset_index(drop=True)\n",
        "\n",
        "             theme = sheet_name\n",
        "\n",
        "             # Identifier tous les subthèmes présents dans l'onglet en respectant la structure corrigée\n",
        "             subthemes = df.iloc[:, 0].dropna().unique()\n",
        "\n",
        "             # Nouvelle identification du périmètre de chaque subthème, en tenant compte de la correction\n",
        "             subthemes_perimeter_corrected = {\n",
        "             \"landside\": [],\n",
        "             \"airside\": [],\n",
        "             \"overall\": []\n",
        "             }\n",
        "\n",
        "             for subtheme in subthemes:\n",
        "                  # Vérifier si le subthème correspond aux catégories connues, sinon l'ajouter à \"overall\"\n",
        "                 if subtheme in landside_subthemes:\n",
        "                     subthemes_perimeter_corrected[\"landside\"].append(subtheme)\n",
        "                 elif subtheme in airside_subthemes:\n",
        "                     subthemes_perimeter_corrected[\"airside\"].append(subtheme)\n",
        "                 elif subtheme in overall_subthemes:\n",
        "                     subthemes_perimeter_corrected[\"overall\"].append(subtheme)\n",
        "\n",
        "             # Fusionner tous les sous-thèmes pour simplifier l'extraction\n",
        "             all_subthemes = subthemes_perimeter_corrected['landside'] + subthemes_perimeter_corrected['airside'] + subthemes_perimeter_corrected['overall']\n",
        "\n",
        "             # Réinitialisation du dictionnaire pour stocker les données correctement extraites pour chaque sous-thème\n",
        "             subtheme_contents = {}\n",
        "             current_subtheme = None\n",
        "             start_index = None\n",
        "\n",
        "             for index, row in df.iterrows():\n",
        "                 # Détection d'un sous-thème\n",
        "                 if row[0] in all_subthemes:\n",
        "                     if current_subtheme is not None:\n",
        "                          # Extraction du contenu du sous-thème précédent jusqu'à \"Aggregate Rating\"\n",
        "                         end_index = df.index[(df[0] == \"Aggregate Rating\") & (df.index > start_index)].min()\n",
        "                         subtheme_contents[current_subtheme] = df.iloc[start_index:end_index].reset_index(drop=True)\n",
        "                     current_subtheme = row[0]\n",
        "                     start_index = index + 1\n",
        "                 elif index == df.shape[0] - 1 and current_subtheme is not None:\n",
        "                     #Gérer le dernier sous-thème\n",
        "                     end_index = df.index[(df[0] == \"Aggregate Rating\") & (df.index > start_index)].min()\n",
        "                     subtheme_contents[current_subtheme] = df.iloc[start_index:end_index].reset_index(drop=True)\n",
        "             # Appliquer la fonction à chaque sous-thème\n",
        "             for subtheme, data in subtheme_contents.items():\n",
        "                 if subtheme in subthemes_perimeter_corrected['landside']:\n",
        "                     add_data_to_df(subtheme, data, 'landside', theme, file_name)\n",
        "                 elif subtheme in subthemes_perimeter_corrected['airside']:\n",
        "                     add_data_to_df(subtheme, data, 'airside', theme, file_name)\n",
        "                 elif subtheme in subthemes_perimeter_corrected['overall']:\n",
        "                     add_data_to_df(subtheme, data, 'overall', theme, file_name)\n",
        "\n",
        "\n",
        "     ################################################################## Table Row\n",
        "     df_landside_raw = df_landside\n",
        "     df_airside_raw = df_airside\n",
        "     df_overall_raw = df_overall\n",
        "\n",
        "     df_landside_overall_raw = df_landside_overall\n",
        "     df_airside_overall_raw = df_airside_overall\n",
        "     df_overall_overall_raw = df_overall_overall\n",
        "\n",
        "     # Conversion des colonnes en numérique\n",
        "     df_landside_raw[['ORY1', 'ORY2', 'ORY3', 'ORY4']] = df_landside_raw[['ORY1', 'ORY2', 'ORY3', 'ORY4']].apply(pd.to_numeric, errors='coerce')\n",
        "     df_airside_raw[['Gates A', 'Gates B', 'Gates C', 'Gates D','Gates E','Gates F']] = df_airside_raw[['Gates A', 'Gates B', 'Gates C', 'Gates D','Gates E','Gates F']].apply(pd.to_numeric, errors='coerce')\n",
        "     df_overall_raw[['Rates']] = df_overall_raw[['Rates']].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "     # Conversion des colonnes en numérique\n",
        "     df_landside_overall_raw[['ORY1', 'ORY2', 'ORY3', 'ORY4']] = df_landside_overall_raw[['ORY1', 'ORY2', 'ORY3', 'ORY4']].apply(pd.to_numeric, errors='coerce')\n",
        "     df_airside_overall_raw[['Gates A', 'Gates B', 'Gates C', 'Gates D','Gates E','Gates F']] = df_airside_overall_raw[['Gates A', 'Gates B', 'Gates C', 'Gates D','Gates E','Gates F']].apply(pd.to_numeric, errors='coerce')\n",
        "     df_overall_overall_raw[['Rates']] = df_overall_overall_raw[['Rates']].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "     # Convertir le DataFrame Pandas en DataFrame Spark\n",
        "     sdf_landside_raw = spark.createDataFrame(df_landside_raw)\n",
        "     sdf_airside_raw = spark.createDataFrame(df_airside_raw)\n",
        "     sdf_overall_raw = spark.createDataFrame(df_overall_raw)\n",
        "\n",
        "     sdf_landside_overall_raw = spark.createDataFrame(df_landside_overall_raw)\n",
        "     sdf_airside_overall_raw = spark.createDataFrame(df_airside_overall_raw)\n",
        "     sdf_overall_overall_raw = spark.createDataFrame(df_overall_overall_raw)\n",
        "\n",
        "     for col in sdf_airside_raw.columns:\n",
        "         new_col_name = col.replace(\" \", \"\")\n",
        "         sdf_airside_raw = sdf_airside_raw.withColumnRenamed(col, new_col_name)\n",
        "\n",
        "     for col in sdf_airside_overall_raw.columns:\n",
        "         new_col_name = col.replace(\" \", \"\")\n",
        "         sdf_airside_overall_raw = sdf_airside_overall_raw.withColumnRenamed(col, new_col_name)\n",
        "\n",
        "     # Ecrire le DataFrame Spark dans Hive\n",
        "     sdf_landside_raw.write.mode(\"append\").saveAsTable(\"c_ing.Skytrax_ORY_Rating_landside\")\n",
        "     sdf_airside_raw.write.mode(\"append\").saveAsTable(\"c_ing.Skytrax_ORY_Rating_airside\")\n",
        "     sdf_overall_raw.write.mode(\"append\").saveAsTable(\"c_ing.Skytrax_ORY_Rating_overall\")\n",
        "\n",
        "     sdf_landside_overall_raw.write.mode(\"append\").saveAsTable(\"c_ing.Skytrax_ORY_Rating_landside_overall\")\n",
        "     sdf_airside_overall_raw.write.mode(\"append\").saveAsTable(\"c_ing.Skytrax_ORY_Rating_airside_overall\")\n",
        "     sdf_overall_overall_raw.write.mode(\"append\").saveAsTable(\"c_ing.Skytrax_ORY_Rating_overall_overall\")\n",
        "\n",
        "     ########################################################################################## Table Exp\n",
        "     # Utilisation de melt pour transformer les colonnes en lignes\n",
        "     df_landside_exp = pd.melt(df_landside_raw, id_vars=['Details', 'Subtheme', 'Theme', 'Plateforme', 'Perimetre', 'Year'],\n",
        "                    value_vars=['ORY1', 'ORY2', 'ORY3', 'ORY4'],\n",
        "                    var_name='Terminal', value_name='Note_Terminal')\n",
        "\n",
        "     # Reorganiser les colonnes\n",
        "     df_landside_exp = df_landside_exp[['Details', 'Note_Terminal', 'Terminal', 'Subtheme', 'Theme', 'Plateforme', 'Perimetre', 'Year']]\n",
        "\n",
        "     # Utilisation de melt pour transformer les colonnes en lignes\n",
        "     df_airside_exp = pd.melt(df_airside_raw, id_vars=['Details', 'Subtheme', 'Theme', 'Plateforme', 'Perimetre', 'Year'],\n",
        "                    value_vars=['Gates A', 'Gates B', 'Gates C', 'Gates D','Gates E','Gates F'],\n",
        "                    var_name='Porte', value_name='Note_Porte')\n",
        "\n",
        "     # Reorganiser les colonnes\n",
        "     df_airside_exp = df_airside_exp[['Details', 'Note_Porte', 'Porte', 'Subtheme', 'Theme', 'Plateforme', 'Perimetre', 'Year']]\n",
        "\n",
        "     df_overall_exp = df_overall_raw[['Details', 'Rates', 'Subtheme', 'Theme', 'Plateforme', 'Perimetre', 'Year']]\n",
        "\n",
        "      # Ajout des colonnes manquantes pour uniformiser les schémas de chaque DataFrame\n",
        "     df_landside_exp['Porte'] = np.nan\n",
        "     df_landside_exp['Note_Porte'] = np.nan\n",
        "     df_landside_exp['Rates'] = np.nan\n",
        "\n",
        "     df_airside_exp['Terminal'] = np.nan\n",
        "     df_airside_exp['Note_Terminal'] = np.nan\n",
        "     df_airside_exp['Rates'] = np.nan\n",
        "\n",
        "     df_overall_exp['Terminal'] = np.nan\n",
        "     df_overall_exp['Note_Terminal'] = np.nan\n",
        "     df_overall_exp['Porte'] = np.nan\n",
        "     df_overall_exp['Note_Porte'] = np.nan\n",
        "\n",
        "     # Concaténer les DataFrames préparés verticalement\n",
        "     df_exp = pd.concat([df_landside_exp, df_airside_exp, df_overall_exp], ignore_index=True)\n",
        "\n",
        "     # Réorganiser les colonnes si nécessaire\n",
        "     columns_order = ['Details','Subtheme', 'Theme', 'Terminal', 'Note_Terminal', 'Porte', 'Note_Porte', 'Rates', 'Plateforme', 'Perimetre', 'Year']\n",
        "     df_exp = df_exp[columns_order]\n",
        "\n",
        "     # Convertir le DataFrame Pandas en DataFrame Spark\n",
        "     sdf_exp = spark.createDataFrame(df_exp)\n",
        "\n",
        "     # Ecrire le DataFrame Spark dans Hive\n",
        "     sdf_exp.write.mode(\"append\").saveAsTable(\"c_exp.Skytrax_Rating\")\n",
        "\n",
        "     ############################################################################### Table View\n",
        "\n",
        "     df_view = df_exp.drop([df_exp.columns[0],df_exp.columns[2]], axis=1)\n",
        "\n",
        "     df_landside_view = df_view.groupby(['Subtheme','Terminal']).agg({\n",
        "     'Note_Terminal': 'mean',\n",
        "     'Plateforme': 'first',\n",
        "     'Perimetre': 'first',\n",
        "     'Year': 'first'\n",
        "     }).reset_index()\n",
        "\n",
        "     # Arrondir à un dixième près\n",
        "     df_landside_view['Note_Terminal'] = df_landside_view['Note_Terminal'].round(1)\n",
        "\n",
        "     df_airside_view = df_view.groupby(['Subtheme','Porte']).agg({\n",
        "     'Note_Porte': 'mean',\n",
        "     'Plateforme': 'first',\n",
        "     'Perimetre': 'first',\n",
        "     'Year': 'first'\n",
        "     }).reset_index()\n",
        "\n",
        "     # Arrondir à un dixième près\n",
        "     df_airside_view['Note_Porte'] = df_airside_view['Note_Porte'].round(1)\n",
        "\n",
        "     #Filtrer pour inclure les lignes ou Perimetre est overall\n",
        "     df_overall_view_filtered = df_view[df_view['Perimetre']=='overall']\n",
        "\n",
        "     df_overall_view = df_overall_view_filtered .groupby('Subtheme').agg({\n",
        "     'Rates': 'mean',\n",
        "     'Plateforme': 'first',\n",
        "     'Perimetre': 'first',\n",
        "     'Year': 'first'\n",
        "     }).reset_index()\n",
        "\n",
        "     # Arrondir à un dixième près\n",
        "     df_overall_view['Rates'] = df_overall_view['Rates'].round(1)\n",
        "\n",
        "     # Ajout des colonnes manquantes pour uniformiser les schémas de chaque DataFrame\n",
        "     df_landside_view['Porte'] = np.nan\n",
        "     df_landside_view['Note_Porte'] = np.nan\n",
        "     df_landside_view['Rates'] = np.nan\n",
        "\n",
        "     df_airside_view['Terminal'] = np.nan\n",
        "     df_airside_view['Note_Terminal'] = np.nan\n",
        "     df_airside_view['Rates'] = np.nan\n",
        "\n",
        "     df_overall_view['Terminal'] = np.nan\n",
        "     df_overall_view['Note_Terminal'] = np.nan\n",
        "     df_overall_view['Porte'] = np.nan\n",
        "     df_overall_view['Note_Porte'] = np.nan\n",
        "\n",
        "     # Concaténer les DataFrames préparés verticalement\n",
        "     df_view_finale = pd.concat([df_landside_view, df_airside_view, df_overall_view], ignore_index=True)\n",
        "     # Réorganiser les colonnes si nécessaire\n",
        "     columns_order = ['Subtheme', 'Terminal', 'Note_Terminal', 'Porte', 'Note_Porte', 'Rates', 'Plateforme', 'Perimetre', 'Year']\n",
        "     df_view_finale = df_view_finale[columns_order]\n",
        "\n",
        "     # Convertir le DataFrame Pandas en DataFrame Spark\n",
        "     sdf_view = spark.createDataFrame(df_view_finale)\n",
        "\n",
        "     # Ecrire le DataFrame Spark dans Hive\n",
        "     sdf_view.write.mode(\"append\").saveAsTable(\"c_rep.Skytrax_Rating\")\n",
        "\n",
        "     ################################################################################################## MAJ de la table des nouveaux fichiers\n",
        "\n",
        "     # Enregistrer les chemins des nouveaux fichiers traités dans Hive\n",
        "     spark.sql(f\"INSERT INTO c_tech.skytrax_treated_files_orly VALUES ('{file_name}')\")\n",
        "else:\n",
        "    print(\"Aucun nouveau  fichier dans cette repertoire, veuillez réessayer utlterièrement\")\n",
        "%sql\n",
        "select * from c_tech.skytrax_treated_files_orly\n",
        "%sql\n",
        "select * from c_rep.Skytrax_Rating\n",
        "%sql\n",
        "select count(*) from c_rep.Skytrax_Rating where Year=\"2024\"\n",
        "%sql\n",
        "select count(*) from c_rep.Skytrax_Rating where Year=\"2021\"\n",
        "%sql\n",
        "select count(*) from c_rep.Skytrax_Rating where Year=\"2022\"\n",
        "%sql\n",
        "select count(*) from c_rep.Skytrax_Rating where Year=\"2021\" and Porte is not null\n",
        "%sql\n",
        "select distinct(Subtheme) from c_rep.Skytrax_Rating where Year=\"2021\" and Terminal is not null order by Subtheme asc"
      ]
    }
  ]
}